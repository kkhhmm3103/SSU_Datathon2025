{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkhhmm3103/SSU_Datathon2025/blob/main/%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5_%E1%84%8C%E1%85%A6%E1%84%8E%E1%85%AE%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "from typing import Union"
      ],
      "metadata": {
        "id": "hfKcZe_XaPQy"
      },
      "id": "hfKcZe_XaPQy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\n",
        "    \"keyword_bert_df.csv\",\n",
        "    encoding=\"utf-8-sig\"\n",
        ")\n",
        "print(df.shape)\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "9MgOrJOzYrTA"
      },
      "id": "9MgOrJOzYrTA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAP_CSV  = \"keyword_map.csv\"     # raw,normalized\n",
        "DROP_CSV = \"keyword_drop.csv\"    # raw\n",
        "\n",
        "def load_keyword_maps(map_csv=MAP_CSV, drop_csv=DROP_CSV):\n",
        "    # --- map ---\n",
        "    m = pd.read_csv(map_csv, encoding=\"utf-8-sig\")\n",
        "    m = m.dropna(subset=[\"raw\", \"normalized\"]).copy()\n",
        "    m[\"raw\"] = m[\"raw\"].astype(str).str.strip()\n",
        "    m[\"normalized\"] = m[\"normalized\"].astype(str).str.strip()\n",
        "\n",
        "    # raw 키는 map_soft에서 normalize한 key로 들어오기 때문에\n",
        "    # 저장된 csv도 같은 기준(소문자/공백/괄호제거)로 맞춰주는 게 안전함\n",
        "    # (아래 normalize 함수 정의 뒤에 다시 한번 정규화 적용함)\n",
        "\n",
        "    map_dict = dict(zip(m[\"raw\"], m[\"normalized\"]))\n",
        "\n",
        "    # --- drop ---\n",
        "    d = pd.read_csv(drop_csv, encoding=\"utf-8-sig\")\n",
        "    d = d.dropna(subset=[\"raw\"]).copy()\n",
        "    d[\"raw\"] = d[\"raw\"].astype(str).str.strip()\n",
        "    drop_set = set(d[\"raw\"].tolist())\n",
        "\n",
        "    return map_dict, drop_set\n",
        "\n",
        "# 전처리(키워드 정규화/표준화) 블록\n",
        "\n",
        "GENERIC_KW = {\"기술\", \"연구\"}\n",
        "REMOVE_EXACT_NORM = {\"기술\", \"technology\", \"연구\"}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 하이픈/슬래시 같은 토큰 내부 기호 처리 정책\n",
        "# - True: U-net, BERT-base, 5G/6G 등 보존\n",
        "# - False: 연결기호를 공백으로 바꿔 토큰 분해 (U-net -> U net)\n",
        "# ------------------------------------------------------------\n",
        "KEEP_INNER_CONNECTORS = True\n",
        "\n",
        "BRACKET_PATTERNS = [r\"\\([^)]*\\)\", r\"\\[[^\\]]*\\]\", r\"\\{[^}]*\\}\", r\"\\<[^>]*\\>\"]\n",
        "MULTISPACE_RE = re.compile(r\"\\s+\")\n",
        "HANGUL_RE = re.compile(r\"[가-힣]\")\n",
        "\n",
        "if KEEP_INNER_CONNECTORS:\n",
        "    # 토큰 내부 연결기호(-,/ 등)는 보존하고, ·• 같은 특수 구분자만 공백 처리\n",
        "    CONNECTORS_RE = re.compile(r\"[·•]+\")\n",
        "else:\n",
        "    CONNECTORS_RE = re.compile(r\"[_\\-–—/\\\\·•]+\")\n",
        "\n",
        "# 공통 유틸\n",
        "def split_keywords(x, sep=\",\"):\n",
        "    if not isinstance(x, str) or not x.strip():\n",
        "        return []\n",
        "    return [t.strip() for t in x.split(sep) if t.strip()]\n",
        "\n",
        "def normalize_kw_basic(x: str) -> str:\n",
        "    x = str(x)\n",
        "    x = x.strip()\n",
        "    x = x.lower()\n",
        "    x = \" \".join(x.split())\n",
        "    return x\n",
        "\n",
        "def remove_parenthetical(x: str) -> str:\n",
        "    # deep learning(딥러닝) -> deep learning\n",
        "    return re.sub(r\"\\s*\\([^)]*\\)\", \"\", str(x)).strip()\n",
        "\n",
        "def basic_cleanup(s: str) -> str:\n",
        "    s = \"\" if s is None else str(s)\n",
        "    s = unicodedata.normalize(\"NFKC\", s).strip()\n",
        "    s = CONNECTORS_RE.sub(\" \", s)   # 정책에 따라 다르게 동작\n",
        "    s = MULTISPACE_RE.sub(\" \", s).strip(\" \\t\\r\\n;|,\")\n",
        "    return s\n",
        "\n",
        "# CSV 매핑 로드 + key 정규화 통일\n",
        "MAP_DICT_RAW, DROP_SET_RAW = load_keyword_maps()\n",
        "\n",
        "def _norm_map_key(k: str) -> str:\n",
        "    # map_soft에서 쓰는 기준과 동일하게\n",
        "    return normalize_kw_basic(remove_parenthetical(k))\n",
        "\n",
        "# CSV에 들어있는 raw 키들도 기준 통일\n",
        "MAP_DICT = {_norm_map_key(k): normalize_kw_basic(v) for k, v in MAP_DICT_RAW.items()}\n",
        "DROP_SET = {_norm_map_key(k) for k in DROP_SET_RAW}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# CSV 기반 매핑 2종\n",
        "# - strict: 원문 그대로 strip만 하고 찾음\n",
        "# - soft  : 괄호 제거 + 소문자/공백 정리 후 찾음\n",
        "# ------------------------------------------------------------\n",
        "def map_strict(x: str) -> str:\n",
        "    if not isinstance(x, str):\n",
        "        return x\n",
        "    key = x.strip()\n",
        "\n",
        "    # key = normalize_kw_basic(key)\n",
        "    return MAP_DICT.get(key, key)\n",
        "\n",
        "def map_soft(x: str) -> str:\n",
        "    if not isinstance(x, str):\n",
        "        return x\n",
        "    key = normalize_kw_basic(remove_parenthetical(x))\n",
        "\n",
        "    # drop 우선\n",
        "    if key in DROP_SET:\n",
        "        return \"\"\n",
        "\n",
        "    # 매핑\n",
        "    out = MAP_DICT.get(key, key)\n",
        "    out = normalize_kw_basic(out)\n",
        "\n",
        "    # 매핑 결과가 drop이면 제거\n",
        "    if out in DROP_SET:\n",
        "        return \"\"\n",
        "    return out\n",
        "\n",
        "def unify_exact_ko_en(x: str) -> str:\n",
        "    return map_soft(x)\n",
        "\n",
        "def extract_parenthetical_preference(raw: str) -> str:\n",
        "    \"\"\"\n",
        "    '영문(한글)'이면 영문(괄호 밖)을 대표로,\n",
        "    '한글(영문)'이면 영문(괄호 안)을 대표로,\n",
        "    그 외는 원문(가능하면 밖 우선)을 유지.\n",
        "    \"\"\"\n",
        "    s = basic_cleanup(raw)\n",
        "    m = re.match(r\"^(.*?)\\((.*?)\\)\\s*$\", s)\n",
        "    if not m:\n",
        "        return s\n",
        "\n",
        "    outside = basic_cleanup(m.group(1))\n",
        "    inside  = basic_cleanup(m.group(2))\n",
        "\n",
        "    out_has_ko = bool(HANGUL_RE.search(outside))\n",
        "    in_has_ko  = bool(HANGUL_RE.search(inside))\n",
        "\n",
        "    # 영문(한글) -> outside(영문)\n",
        "    if (not out_has_ko) and in_has_ko:\n",
        "        return outside\n",
        "\n",
        "    # 한글(영문) -> inside(영문)\n",
        "    if out_has_ko and (not in_has_ko):\n",
        "        return inside\n",
        "\n",
        "    # outside 우선\n",
        "    return outside if outside else inside\n",
        "\n",
        "def normalize_english_key(s: str) -> str:\n",
        "    s = basic_cleanup(s).lower()\n",
        "    s = MULTISPACE_RE.sub(\" \", s).strip()\n",
        "\n",
        "    s = s.replace(\"analyses\", \"analysis\")\n",
        "    s = s.replace(\"modelling\", \"modeling\")\n",
        "\n",
        "    # s 제거 예외 어미들\n",
        "    NO_STRIP_SUFFIX = (\"sis\", \"us\", \"is\")\n",
        "\n",
        "    if (\n",
        "        s.endswith(\"s\")\n",
        "        and len(s) > 5\n",
        "        and not s.endswith(\"ss\")\n",
        "        and not s.endswith(NO_STRIP_SUFFIX)\n",
        "    ):\n",
        "        s = s[:-1]\n",
        "    return s\n",
        "\n",
        "def canon_key(s: str) -> str:\n",
        "    \"\"\"\n",
        "    자동매핑에서 같은 그룹인지 판단하는 key.\n",
        "    - 괄호 병기 정리 + 브라켓류 제거 + 공백 정리\n",
        "    - 영문은 normalize_english_key로 정규화\n",
        "    - 마지막에 map_soft로 사전 기반 표준화까지 적용\n",
        "    \"\"\"\n",
        "    s = extract_parenthetical_preference(s)\n",
        "\n",
        "    for pat in BRACKET_PATTERNS:\n",
        "        s = re.sub(pat, \" \", s)\n",
        "\n",
        "    s = basic_cleanup(s)\n",
        "\n",
        "    if not HANGUL_RE.search(s):\n",
        "        key = normalize_english_key(s)\n",
        "    else:\n",
        "        key = normalize_kw_basic(s)\n",
        "\n",
        "    # CSV 기반 표준화\n",
        "    key = map_soft(key)\n",
        "    key = normalize_kw_basic(key)\n",
        "    return key\n",
        "\n",
        "def representative_of_group(items):\n",
        "    # 대표어 우선순위: (1) 한글 포함 (2) 괄호 없는 것 (3) 빈도 높은 것\n",
        "    def score(x):\n",
        "        raw, cnt = x\n",
        "        has_ko = 1 if HANGUL_RE.search(raw) else 0\n",
        "        no_paren = 1 if \"(\" not in raw and \")\" not in raw else 0\n",
        "        return (has_ko, no_paren, cnt)\n",
        "    return max(items, key=score)[0]\n",
        "\n",
        "def preprocess_and_aggregate_docs(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    문서 집계용 전처리:\n",
        "    - KYWD explode\n",
        "    - 기본 정규화 + CSV(map_soft) 표준화\n",
        "    - GENERIC_KW 제거\n",
        "    - PBSH에서 year/month\n",
        "    - NODE_ID 단위로 keywords 리스트 집계\n",
        "    \"\"\"\n",
        "    df_kw = df.dropna(subset=[\"KYWD\"]).copy()\n",
        "    df_kw[\"KYWD\"] = df_kw[\"KYWD\"].astype(str).str.split(\",\")\n",
        "    df_kw[\"KYWD\"] = df_kw[\"KYWD\"].apply(lambda xs: [k.strip() for k in xs if k and k.strip()])\n",
        "\n",
        "    df_exp = df_kw.explode(\"KYWD\").dropna(subset=[\"KYWD\"]).copy()\n",
        "\n",
        "    df_exp[\"kw_norm\"] = df_exp[\"KYWD\"].apply(normalize_kw_basic)\n",
        "    df_exp[\"kw_std\"]  = df_exp[\"kw_norm\"].apply(unify_exact_ko_en)\n",
        "\n",
        "    # drop 처리로 빈 문자열 들어올 수 있음\n",
        "    df_exp = df_exp[df_exp[\"kw_std\"].astype(str).str.strip().ne(\"\")].copy()\n",
        "\n",
        "    df_exp = df_exp[~df_exp[\"kw_std\"].isin(GENERIC_KW)].copy()\n",
        "\n",
        "    if \"PBSH\" in df_exp.columns:\n",
        "        df_exp[\"PBSH_str\"] = df_exp[\"PBSH\"].astype(str)\n",
        "        df_exp[\"year\"] = df_exp[\"PBSH_str\"].str[:4]\n",
        "        df_exp[\"month\"] = df_exp[\"PBSH_str\"].str[4:6]\n",
        "    else:\n",
        "        df_exp[\"year\"] = None\n",
        "        df_exp[\"month\"] = None\n",
        "\n",
        "    cols = df_exp.columns\n",
        "\n",
        "    def safe_first(col):\n",
        "        return (col, \"first\") if col in cols else (col, lambda x: None)\n",
        "\n",
        "    doc_df = (\n",
        "        df_exp.groupby(\"NODE_ID\", as_index=False)\n",
        "        .agg(\n",
        "            NODE_TTLE=safe_first(\"NODE_TTLE\"),\n",
        "            NODE_TTLE_EN=safe_first(\"NODE_TTLE_EN\"),\n",
        "            NODE_CLSS_01=safe_first(\"NODE_CLSS_01\"),\n",
        "            NODE_CLSS_02=safe_first(\"NODE_CLSS_02\"),\n",
        "            ABST_KR=safe_first(\"ABST_KR\"),\n",
        "            ABST_EN=safe_first(\"ABST_EN\"),\n",
        "            year=(\"year\", \"first\"),\n",
        "            month=(\"month\", \"first\"),\n",
        "            keywords=(\"kw_std\", lambda x: sorted(set([k for k in x if str(k).strip()]))),\n",
        "            keywords_raw=(\"KYWD\", lambda x: sorted(set([k for k in x if str(k).strip()]))),\n",
        "        )\n",
        "    )\n",
        "    return doc_df\n",
        "\n",
        "\n",
        "# 파이프라인(빈도표 → 자동매핑 → 정규화 적용 → Gephi)\n",
        "def build_keyword_pipeline(\n",
        "    in_data: Union[str, pd.DataFrame],\n",
        "    sep: str = \",\",\n",
        "    out_freq_raw: str = \"keyword_frequency.csv\",\n",
        "    min_count_for_mapping: int = 2,\n",
        "    out_map: str = \"keyword_mapping_auto.csv\",\n",
        "    out_merged_freq: str = \"keyword_frequency_merged.csv\",\n",
        "    save_normalized_dataset: bool = True,\n",
        "    out_data_norm: str = \"dataset_kw_normalized.csv\",\n",
        "    top_n: int = 3000,\n",
        "    min_edge_w: int = 4,\n",
        "    max_kw_per_doc: int = 12,\n",
        "    out_nodes: str = \"gephi_nodes.csv\",\n",
        "    out_edges: str = \"gephi_edges.csv\",\n",
        "):\n",
        "    # 입력 처리: DataFrame vs CSV\n",
        "    if isinstance(in_data, pd.DataFrame):\n",
        "        df_base = in_data.copy()\n",
        "    else:\n",
        "        df_base = pd.read_csv(in_data, encoding=\"utf-8-sig\")\n",
        "\n",
        "    # 원표기 빈도표 생성\n",
        "    df_raw = df_base[df_base[\"KYWD\"].notna()].copy()\n",
        "    df_raw[\"KW_LIST_RAW\"] = df_raw[\"KYWD\"].apply(lambda x: split_keywords(x, sep=sep))\n",
        "\n",
        "    all_keywords_raw = [k for kws in df_raw[\"KW_LIST_RAW\"] for k in kws]\n",
        "    kw_freq_raw = Counter(all_keywords_raw)\n",
        "\n",
        "    freq_raw_df = (\n",
        "        pd.DataFrame(kw_freq_raw.items(), columns=[\"keyword\", \"count\"])\n",
        "        .sort_values(\"count\", ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    freq_raw_df.to_csv(out_freq_raw, index=False, encoding=\"utf-8-sig\")\n",
        "    print(\"[PIPE] saved raw freq:\", out_freq_raw, \"/ unique:\", len(freq_raw_df))\n",
        "\n",
        "    # 자동 매핑 생성 (count>=min_count_for_mapping)\n",
        "    freq_df = freq_raw_df[freq_raw_df[\"count\"] >= min_count_for_mapping].copy()\n",
        "    print(f\"[PIPE] mapping targets (count>={min_count_for_mapping}):\", len(freq_df))\n",
        "\n",
        "    groups = defaultdict(list)\n",
        "    for kw, cnt in zip(freq_df[\"keyword\"], freq_df[\"count\"]):\n",
        "        ck = canon_key(kw)  # CSV 기반 표준화 포함\n",
        "        groups[ck].append((kw, int(cnt)))\n",
        "\n",
        "    rows = []\n",
        "    merged = []\n",
        "    for ck, items in groups.items():\n",
        "        rep_raw = representative_of_group(items)\n",
        "        rep_pref = extract_parenthetical_preference(rep_raw)\n",
        "\n",
        "        # canon_key()가 CSV 기반 표준화 포함\n",
        "        rep_std = canon_key(rep_pref)\n",
        "        rep_std = normalize_kw_basic(rep_std)\n",
        "\n",
        "        total = sum(c for _, c in items)\n",
        "        merged.append((rep_std, total, len(items)))\n",
        "\n",
        "        for raw, _ in items:\n",
        "            rows.append((raw, rep_std))\n",
        "\n",
        "    map_df = pd.DataFrame(rows, columns=[\"raw\", \"normalized\"]).drop_duplicates()\n",
        "    map_df.to_csv(out_map, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    merged_df = (\n",
        "        pd.DataFrame(merged, columns=[\"normalized\", \"count\", \"variants\"])\n",
        "        .groupby(\"normalized\", as_index=False)\n",
        "        .agg({\"count\": \"sum\", \"variants\": \"sum\"})\n",
        "        .sort_values(\"count\", ascending=False)\n",
        "    )\n",
        "    merged_df.to_csv(out_merged_freq, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(\"[PIPE] saved map:\", out_map, \"/ rows:\", len(map_df))\n",
        "    print(\"[PIPE] saved merged freq:\", out_merged_freq, \"/ unique normalized:\", len(merged_df))\n",
        "\n",
        "    # 원본에 매핑 적용(정규화 키워드 리스트 생성)\n",
        "    map_dict = dict(zip(map_df[\"raw\"], map_df[\"normalized\"]))\n",
        "\n",
        "    def apply_mapping_list(kw_list):\n",
        "        out = [map_dict.get(k, k) for k in kw_list]\n",
        "        out = [k for k in out if isinstance(k, str) and k.strip()]\n",
        "\n",
        "        # 사전(CSV) 기반 표준화까지 적용\n",
        "        out = [map_soft(k) for k in out]\n",
        "        out = [k for k in out if isinstance(k, str) and k.strip()]\n",
        "\n",
        "        # 정확히 일치하는 키워드만 제거\n",
        "        out = [k for k in out if normalize_kw_basic(k) not in REMOVE_EXACT_NORM]\n",
        "\n",
        "        return sorted(set(out))\n",
        "\n",
        "    df_norm = df_base.copy()\n",
        "    df_norm[\"KW_LIST_RAW\"] = df_norm[\"KYWD\"].apply(lambda x: split_keywords(x, sep=sep))\n",
        "    df_norm[\"KW_LIST_NORM\"] = df_norm[\"KW_LIST_RAW\"].apply(apply_mapping_list)\n",
        "    df_norm[\"KYWD_NORM\"] = df_norm[\"KW_LIST_NORM\"].apply(lambda xs: \", \".join(xs))\n",
        "\n",
        "    if save_normalized_dataset:\n",
        "        df_norm.to_csv(out_data_norm, index=False, encoding=\"utf-8-sig\")\n",
        "        print(\"[PIPE] saved normalized dataset:\", out_data_norm)\n",
        "\n",
        "    # Gephi 네트워크 생성\n",
        "    all_kws_norm = [k for kws in df_norm[\"KW_LIST_NORM\"] for k in kws]\n",
        "    freq_norm = Counter(all_kws_norm)\n",
        "\n",
        "    top_keywords = [k for k, _ in freq_norm.most_common(top_n)]\n",
        "    top_set = set(top_keywords)\n",
        "\n",
        "    print(\"[PIPE] unique keywords (norm):\", len(freq_norm))\n",
        "    print(\"[PIPE] top_n nodes:\", len(top_set))\n",
        "\n",
        "    edge_counter = Counter()\n",
        "    for kw_list in df_norm[\"KW_LIST_NORM\"]:\n",
        "        kws = [k for k in kw_list if k in top_set]\n",
        "        kws = sorted(set(kws), key=lambda x: freq_norm[x], reverse=True)[:max_kw_per_doc]\n",
        "        if len(kws) < 2:\n",
        "            continue\n",
        "\n",
        "        for a, b in itertools.combinations(sorted(kws), 2):\n",
        "            edge_counter[(a, b)] += 1\n",
        "\n",
        "    edges = [(a, b, w) for (a, b), w in edge_counter.items() if w >= min_edge_w]\n",
        "    edge_df = (\n",
        "        pd.DataFrame(edges, columns=[\"Source\", \"Target\", \"Weight\"])\n",
        "        .sort_values(\"Weight\", ascending=False)\n",
        "    )\n",
        "    edge_df.to_csv(out_edges, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    node_df = (\n",
        "        pd.DataFrame([(k, freq_norm[k]) for k in top_set], columns=[\"Id\", \"Frequency\"])\n",
        "        .sort_values(\"Frequency\", ascending=False)\n",
        "    )\n",
        "    node_df.to_csv(out_nodes, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(\"[PIPE] saved edges:\", out_edges, \"/ edges:\", len(edge_df))\n",
        "    print(\"[PIPE] saved nodes:\", out_nodes, \"/ nodes:\", len(node_df))\n",
        "\n",
        "    return {\n",
        "        \"freq_raw_df\": freq_raw_df,\n",
        "        \"map_df\": map_df,\n",
        "        \"merged_df\": merged_df,\n",
        "        \"df_norm\": df_norm,\n",
        "        \"node_df\": node_df,\n",
        "        \"edge_df\": edge_df,\n",
        "    }\n",
        "\n",
        "results = build_keyword_pipeline(\n",
        "    in_data=df,\n",
        "    sep=\",\",\n",
        "    out_freq_raw=\"keyword_frequency333.csv\",\n",
        "    min_count_for_mapping=2,\n",
        "    out_map=\"keyword_mapping_auto_countge2_v2333.csv\",\n",
        "    out_merged_freq=\"keyword_frequency_merged_countge2_v2333.csv\",\n",
        "    save_normalized_dataset=True,\n",
        "    out_data_norm=\"keyword_bert_df.csv\",\n",
        "    top_n=3000,\n",
        "    min_edge_w=4,\n",
        "    max_kw_per_doc=12,\n",
        "    out_nodes=\"gephi_nodes_topN333.csv\",\n",
        "    out_edges=\"gephi_edges_topN333.csv\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUHWX9DVNIBU",
        "outputId": "07918c46-b00d-4287-f4f0-607ed419d76d"
      },
      "id": "gUHWX9DVNIBU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [PIPE] saved raw freq: keyword_frequency333.csv / unique: 178391\n",
            "✅ [PIPE] mapping targets (count>=2): 31935\n",
            "✅ [PIPE] saved map: keyword_mapping_auto_countge2_v2333.csv / rows: 31935\n",
            "✅ [PIPE] saved merged freq: keyword_frequency_merged_countge2_v2333.csv / unique normalized: 25242\n",
            "✅ [PIPE] saved normalized dataset: keyword_bert_df.csv\n",
            "✅ [PIPE] unique keywords (norm): 150645\n",
            "✅ [PIPE] top_n nodes: 3000\n",
            "✅ [PIPE] saved edges: gephi_edges_topN333.csv / edges: 2268\n",
            "✅ [PIPE] saved nodes: gephi_nodes_topN333.csv / nodes: 3000\n",
            "===== DONE =====\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}